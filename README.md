# ğŸ“Œ Retrieval-Augmented Generation (RAG) Chatbot

A GenAI chatbot that uses RAG to generate accurate and context-aware answers using
FAISS vector search + LLMs.

## ğŸš€ Features
- Document ingestion and chunking
- Embedding generation using HuggingFace
- FAISS vector database for semantic search
- RAG pipeline: Retrieve â†’ Generate
- Flask API for chatbot usage

## ğŸ—‚ï¸ Project Structure
rag-chatbot/
â”‚â”€â”€ app.py
â”‚â”€â”€ ingest.py
â”‚â”€â”€ vector_store/
â”‚â”€â”€ data/
â”‚   â””â”€â”€ documents.pdf
â”‚â”€â”€ requirements.txt

## ğŸ§  How It Works
1. Documents are split into chunks  
2. Embeddings created using HF models  
3. Stored in FAISS vector DB  
4. User query â†’ semantic search â†’ nearest chunks  
5. LLM generates answer using retrieved context  

## â–¶ï¸ Run
pip install -r requirements.txt  
python ingest.py  
python app.py

## ğŸ“˜ Model Used
- HuggingFace Embeddings
- OpenAI GPT / HuggingFace LLM
